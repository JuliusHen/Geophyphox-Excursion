{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "induced-sussex",
   "metadata": {},
   "source": [
    "# Magnetic Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunrise-sandwich",
   "metadata": {},
   "source": [
    "In this notebook you will undertake the data processing steps that are necessary in order to produce a magnetic anomaly map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affected-facility",
   "metadata": {},
   "source": [
    "## Importing external libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-castle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from scipy import interpolate\n",
    "import pyproj\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-defense",
   "metadata": {},
   "source": [
    "## Importing and visualizing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "international-compound",
   "metadata": {},
   "source": [
    "The next cells will load the data. \n",
    "You will need change the file-path as well as your student id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a97e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the path to your data folder here\n",
    "filename = Path(r'C:\\Users\\julik\\sciebo\\Master\\Geophyphox\\Excursion 2022\\Student Data\\Raw') \n",
    "\n",
    "# Enter the student_id you used in the field\n",
    "student_id = \"300724\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nutritional-librarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the dataset\n",
    "data = pd.read_csv(os.path.join(filename, \"mag_data_\"+str(student_id)+\".csv\"))\n",
    "\n",
    "# Renaming some columns for easier coding later on \n",
    "data = data.rename(columns={'mag_abs':'field'})\n",
    "\n",
    "# The magnetic field values are stored in µT. Since the convention is to display data in nT, we modify accordingly\n",
    "data['field'] *= 1000\n",
    "\n",
    "# Projecting coordinates\n",
    "projection = pyproj.Proj(proj='utm',zone=32, lat_ts=data['lat'].mean()) #'merc',proj='utm',zone=32,ellps='WGS84'\n",
    "# Project the dataset coordinates:\n",
    "easting, northing = projection(np.array(data['lon']), np.array(data['lat']))\n",
    "\n",
    "data['x'] = easting\n",
    "data['y'] = northing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-physiology",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Taking a first look at the dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-hartford",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We can also call some quick summary statistics in order to get a first feel for our data\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "found-jamaica",
   "metadata": {},
   "source": [
    "The time values in the dataset refer to the elapsed time in seconds since the experiment started. Since we want to combine all phones' measurements later on, it is necessary to normalise the time values to some standard reference time, like 0:00 on the measurement date. The absolute start time of the experiment is saved as [unix-timestamp](https://www.unixtimestamp.com). This timestamp can be converted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-hindu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a new column to the dataset in which the normalized time is stored.\n",
    "data[\"times\"] = np.zeros(len(data))\n",
    "for count,unix in enumerate(data[\"unix_time\"].values):\n",
    "    t_0_str = datetime.datetime.fromtimestamp(int(unix)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    t_0_str = t_0_str[11:]\n",
    "    t_0 = int(t_0_str[:2])*3600 # Converting hours to seconds\n",
    "    t_0 += int(t_0_str[3:5])*60 # Converting minutes to seconds\n",
    "    t_0 += float(t_0_str[6:]) # Adding remaining seconds\n",
    "    data[\"times\"].iloc[count]=t_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-powder",
   "metadata": {},
   "source": [
    "If, for some reason, one row in the dataset is incomplete (i.e. missing coordinate values), we can not usefully interpret said datapoint. In the next step, we remove all rows that contain missing, or NaN, values. In the dataframe description below, every column should then have the same amount of entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-enhancement",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Removin NaN values\n",
    "data.dropna(inplace=True)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-diesel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting all datapoints in order of appearance in the dataset\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "nData = len(data['field'])\n",
    "ax.scatter(data['times'], data['field'])\n",
    "ax.set(xlabel='Measurement Time [s]', ylabel='B [nT]', title=f'Original dataset with {nData} measurements.');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naked-conviction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the collected data spatially\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Using the temporally corrected anomalies\n",
    "im = ax.scatter(data['x'], data['y'], c=data['field'])\n",
    "ax.set(xlabel='Easting', ylabel='Northing', title='Collected data')\n",
    "ax.set_xticklabels(['{:}'.format(int(_)) for _ in ax.get_xticks().tolist()]) # Fixes x-axis tick labels\n",
    "ax.set_yticklabels(['{:}'.format(int(_)) for _ in ax.get_yticks().tolist()]) # Fixes y-axis tick labels\n",
    "plt.colorbar(im, ax=ax, label='Magnetic field [T]');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-escape",
   "metadata": {},
   "source": [
    "## Getting accurate altitude values\n",
    "\n",
    "The altitudes recorded with your phones' GPS are likely imprecise. We can get accurate altitude values from a DEM of the survey area:\n",
    "Enter the path where the DEM file is located on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-karen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the DEM\n",
    "dem_location = r'C:\\Users\\julik\\sciebo\\Master\\Geophyphox\\Excursion 2022\\DEM\\Dürwiss_combined.txt'\n",
    "dem = pd.read_csv(dem_location, sep=' ', names=['easting', 'northing', 'altitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-vancouver",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dem = []\n",
    "\n",
    "for _x, _y in zip(data['x'], data['y']):\n",
    "    _x = round(_x, 0)\n",
    "    _y = round(_y, 0)\n",
    "    mask = np.logical_and(dem['easting'] == _x, dem['northing'] == _y)\n",
    "    p = dem.loc[mask]\n",
    "    z_dem.append(p['altitude'].values[0].copy())\n",
    "    \n",
    "data['z_dem'] = z_dem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-surfing",
   "metadata": {},
   "source": [
    "## Handling Outliers\n",
    "\n",
    "We can now search the dataset for any outliers. There are many ways of defining and identifying outliers, here we will define a threshold value above or below which we want to discard any values. The median of the data will be our point of reference, and any field value lying above or below a predefined range around the median will be discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-raleigh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we set the unilateral range around the median, which defines the window of values we want to keep\n",
    "rangespan = 1300\n",
    "median_data = data['field'].median()\n",
    "\n",
    "# Creating a vector with the absolute differences between each field value and the median\n",
    "diff_median = np.abs(data['field'] - median_data)\n",
    "\n",
    "# Storing all datapoints that do not meet the outlier criterion\n",
    "data_outlier = data.loc[diff_median < rangespan]\n",
    "\n",
    "# This new dataframe is now void of outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automatic-guard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can plot the dataset with outliers removed\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "nDataOutlier = len(data_outlier['field'])\n",
    "ax.scatter(data['times'], data['field'])\n",
    "ax.hlines(median_data, data['times'].min()-500, data['times'].max()+500, color='purple', label='Data median')\n",
    "ax.hlines(median_data+rangespan, data['times'].min()-500, data['times'].max()+500,\n",
    "          linestyles='dashed', color='red', label='Upper outlier bound')\n",
    "ax.hlines(median_data-rangespan, data['times'].min()-500, data['times'].max()+500,\n",
    "          linestyles='dashed', color='blue', label='Lower outlier bound')\n",
    "ax.set(xlabel='Measurement Time [s]', ylabel='B [nT]',\n",
    "       title=f'Outlier removed dataset with {nDataOutlier} measurements, {nData-nDataOutlier} measurements were discarded');\n",
    "ax.legend()\n",
    "#plt.xticks(ticks=range(0, len(times_outlier), 200), labels=times_outlier[::200], rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-algeria",
   "metadata": {},
   "source": [
    "## Data Correction\n",
    "\n",
    "To obtain magnetic anomalies, we need to extract the anomal part of the data. This can be done by subtracting a calculated (or measured) magnetic field value from the data. Another method is to also correct for the temporal drift which naturally occurs, for which measurements at a base station are needed. In our case, we can also use the smartphone measurements taken at the reference location as base station data. Using these data will also eliminate any measurement deviations that occur between smartphones.\n",
    "\n",
    "Note that only one appropriate method must be chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-bacon",
   "metadata": {},
   "source": [
    "## Getting Base Station Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-tyler",
   "metadata": {},
   "source": [
    "If we want to use smartphone measurements as base station values, we need to get the values that were taken at the reference point. Since GPS measurements are not entirely precise, we define a tolerance range around which we will accept measurements as base station values. Of course we also have to specify the reference points' coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-democracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the reference location and tolerance\n",
    "# Using first measurement locaion as default\n",
    "refPointX = data_outlier['x'][0]\n",
    "refPointY = data_outlier['y'][0]\n",
    "\n",
    "# Defining the tolerance\n",
    "tolX = 15\n",
    "tolY = 15\n",
    "\n",
    "refMinX, refMaxX = (refPointX-tolX, refPointX+tolX)\n",
    "refMinY, refMaxY = (refPointY-tolY, refPointY+tolY)\n",
    "\n",
    "maskX = np.logical_and(data_outlier['x'] < refMaxX, data_outlier['x'] > refMinX)\n",
    "maskY = np.logical_and(data_outlier['y'] < refMaxY, data_outlier['y'] > refMinY)\n",
    "maskAll = np.logical_and(maskX, maskY)\n",
    "\n",
    "data_base = data_outlier.loc[maskAll]\n",
    "nDataBase = len(data_base['x'])\n",
    "\n",
    "# Plotting reference measurements\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "im = ax.scatter(data_outlier['x'], data_outlier['y'], c=data_outlier['field'], clip_on=True, label='Measurements')\n",
    "ax.set(xlabel='Easting', ylabel='Northing', title=f'Magnetic field map with {nDataBase} reference points.')\n",
    "refArea = mpl.patches.Rectangle((refMinX, refMinY), width=refMaxX-refMinX, height=refMaxY-refMinY,\n",
    "                               fill=False, label='Reference point selection')\n",
    "ax.set_yticklabels(['{:}'.format(int(_)) for _ in ax.get_yticks().tolist()]) # Fixes y-axis tick labels\n",
    "ax.add_artist(refArea)\n",
    "im_ref = ax.scatter(refPointX, refPointY, c='r', zorder=3, label='Reference point')\n",
    "plt.colorbar(im, ax=ax, label='Magnetic field [nT]')\n",
    "ax.legend(handles=[im, im_ref, refArea]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-aspect",
   "metadata": {},
   "source": [
    "If the reference area includes less than 2 points, increase the tolerance. Once the base station values are obtained, they can be used in the time drift correction. Set the variable below to 'temporal' in order to use this type of corrrection. For a theoretical reference correction you can also use enter 'reference'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-paste",
   "metadata": {},
   "source": [
    "## Altitude correction \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-scotland",
   "metadata": {},
   "source": [
    "There is an altitude dependency of the magnetic field. This correction is undertaken before any other calculations are done, and will be done on the basis of a digital elevation model (DEM) of the area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-mandate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining reference height for altitude correction\n",
    "# Using first measurement location as reference\n",
    "RefHeight = data_base['z_dem'][0]\n",
    "dH = data_outlier['z_dem'] - RefHeight\n",
    "R = RefHeight + 6371000 # (earth radius in m)\n",
    "field_altitude = data_outlier['field']*(1+(dH/R))**(-3)\n",
    "data_outlier['field_alt'] = field_altitude\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-appreciation",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "ax.plot(data_outlier['times'], data_outlier['field'], '--', label='Raw data')\n",
    "ax.plot(data_outlier['times'], data_outlier['field_alt'], label='Altitude corrected data')\n",
    "ax.set(xlabel='Measurement time [s]', ylabel='Magnetic field B [nT]')\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-responsibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_outlier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-burke",
   "metadata": {},
   "source": [
    "## Determining the anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-marriage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select here how you want to calculate anomalies: 'reference' or 'temporal'\n",
    "anomaly_method = 'reference'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biblical-workshop",
   "metadata": {},
   "source": [
    "### Time drift correction\n",
    "\n",
    "The earth's magnetic field intensity changes throughout the day. If there is data available from a base station, the temporal drift can be measured and subtracted from the measured data. The base station data then also directly serves as a total field reference value, which means that the resulting values represent the magnetic anomaly relative to the base station location.\n",
    "The anomaly can also be calculated by using a local theoretical reference value, which is demonstrated in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-argument",
   "metadata": {},
   "outputs": [],
   "source": [
    "if anomaly_method == 'temporal':\n",
    "    # In order to get basestation values for the exact times at which our actual measurements were taken,\n",
    "    # we resample the basestation data. This is done via linear interpolation.\n",
    "\n",
    "    baseline_interpolator = interpolate.interp1d(data_base['times'], data_base['field'], kind='linear',\n",
    "                                                 fill_value='extrapolate')\n",
    "    baseline_field = baseline_interpolator(data_outlier['times'])\n",
    "\n",
    "    field_anomaly = data_outlier['field'] - baseline_field\n",
    "    \n",
    "    # Adding anomalies to dataframe\n",
    "    data_outlier['anomaly'] = field_anomaly\n",
    "    \n",
    "    # Plotting the results\n",
    "\n",
    "    fig, (ax0, ax1) = plt.subplots(2, 1, figsize=(12,8), sharex=True)\n",
    "    \n",
    "    ax1.scatter(data_outlier['times'], data_outlier['anomaly'])\n",
    "    ax1.set(xlabel='Measurement Time [h]', ylabel='B [nT]', title='Temporally corrected magnetic anomalies')\n",
    "    \n",
    "    ax0.scatter(data_base['times'], data_base['field'], label='Base station data')\n",
    "    ax0.plot(data_outlier['times'], baseline_field, 'k', label='Interpolated time drift')\n",
    "    ax0.set(title='Base station measurements', ylabel='Magnetic field B [nT]')\n",
    "    ax0.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legendary-excerpt",
   "metadata": {},
   "source": [
    "### Substracting Reference Fiel\n",
    "If there is no base station data available that can act as a local reference value, a theoretical backround value can be determined. Models for the earth's theoretical magnetic field are available, such as the [World Magnetic Model](http://geomag.bgs.ac.uk/data_service/models_compass/wmm_calc.html), which calculate the theoretical field value B for a given location. This can then be used to calculate deviations between our measurements and the 'ideal' value, returning the desired anomaly without accounting for any temporal trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-submission",
   "metadata": {},
   "outputs": [],
   "source": [
    "if anomaly_method == 'reference':\n",
    "    # Here we define the background value for the correction, taken from literature or a suitable model\n",
    "    reference = data_base['field'].mean() # nT\n",
    "\n",
    "    field_anomaly = data_outlier['field'] - reference\n",
    "    data_outlier['anomaly'] = field_anomaly\n",
    "    # Plotting the results\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "    ax.scatter(data_outlier['times'], data_outlier['anomaly'])\n",
    "    ax.set(xlabel='Measurement Time [s]', ylabel='B [nT]', title='Theoretical magnetic anomalies')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e195c4cf",
   "metadata": {},
   "source": [
    "## Plotting all data\n",
    "\n",
    "We can now visualize all collected data. Starting with a scatterplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8114242b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Anomaly map\n",
    "fig, (ax0, ax1, ax2) = plt.subplots(3, 1, figsize=(14, 12))\n",
    "\n",
    "vmin = -np.max(np.abs(data_outlier['anomaly']))\n",
    "vmax = np.max(np.abs(data_outlier['anomaly']))\n",
    "\n",
    "# Using the temporally corrected anomalies\n",
    "im0 = ax0.scatter(data_outlier['x'], data_outlier['y'], c=data_outlier['field'])\n",
    "namegroups = data_base.groupby('student_id_list')\n",
    "for name, group in namegroups:\n",
    "    ax1.plot(group['times'], group['field'], '-o', label=name)\n",
    "ax1.legend()\n",
    "im2 = ax2.scatter(data_outlier['x'], data_outlier['y'], c=data_outlier['anomaly'],\n",
    "                  vmin=vmin, vmax=vmax, cmap=\"RdBu_r\")\n",
    "\n",
    "ax0.set(xlabel='Easting', ylabel='Northing', title='Outlier & altitude corrected data')\n",
    "ax1.set(xlabel='Measurement time [s]', ylabel='Magnetic field B [nT]', title='Base station data')\n",
    "ax2.set(xlabel='Easting', ylabel='Northing', title='Field anomaly map')\n",
    "plt.colorbar(im0, ax=ax0, label='Magnetic field B [nT]');\n",
    "#plt.colorbar(im1, ax=ax1, label='Magnetic field B [nT]');\n",
    "plt.colorbar(im2, ax=ax2, label='Magnetic anomaly [nT]');\n",
    "\n",
    "ax0.set_yticklabels(['{:}'.format(int(x)) for x in ax0.get_yticks().tolist()]) # Fixes y-axis tick labels\n",
    "ax2.set_yticklabels(['{:}'.format(int(x)) for x in ax2.get_yticks().tolist()]) # Fixes y-axis tick labels\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-shirt",
   "metadata": {},
   "source": [
    "## Saving the results\n",
    "\n",
    "As a last step, we save our results. There are a multitude of ways and fileformats to achieve this. Since we want to store numerical data, [Numpy's save function](https://numpy.org/doc/stable/reference/generated/numpy.savez.html) is a fast and effective method to achieve this. We save the calculated anomalies as well as all data processing steps in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-colors",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data. Give your datafile a name so it can be distinguished from the others.\n",
    "\n",
    "fileName = 'results_magnetic_' + student_id + '.csv'\n",
    "baseFileName = 'results_magneticBASE_' + student_id + '.csv'\n",
    "\n",
    "#Enter path where data should be exported too\n",
    "folder = Path(r'C:\\Users\\julik\\sciebo\\Master\\Geophyphox\\Excursion 2022\\Student Data\\Processed')\n",
    "data_outlier['name'] = [student_id]*len(data_outlier)\n",
    "\n",
    "data_base['name'] = [student_id]*len(data_base)\n",
    "data_outlier.to_csv(folder / fileName)\n",
    "data_base.to_csv(folder/ baseFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05454b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "b7a4c45de720de302e52c90c1b44e72492ab9c17588583e112e6be2347e0e93b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
